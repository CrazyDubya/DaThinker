"""
Adaptive Engine Core

The main engine that orchestrates threat detection, context management,
and adaptive response generation.
"""

import uuid
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, List, Callable
from datetime import datetime
from enum import Enum

from .context import ConversationContext, ConversationState, TrustLevel, Message
from .detector import ThreatDetector, ThreatAnalysis, ThreatType


class ResponseMode(Enum):
    """How the engine should respond based on context."""
    NORMAL = "normal"
    CAUTIOUS = "cautious"
    GUARDED = "guarded"
    DEFLECTING = "deflecting"
    REFUSING = "refusing"


@dataclass
class AdaptiveResponse:
    """Response generated by the adaptive engine."""
    content: str
    mode: ResponseMode
    threat_analysis: Optional[ThreatAnalysis] = None
    context_state: Optional[ConversationState] = None
    trust_level: Optional[TrustLevel] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    blocked: bool = False
    reason: str = ""


class AdaptiveEngine:
    """
    The core adaptive conversation engine.

    Integrates threat detection, context management, and response
    adaptation to handle various input types safely.
    """

    # Response templates for different modes
    RESPONSE_TEMPLATES = {
        ResponseMode.NORMAL: "I understand. {response}",
        ResponseMode.CAUTIOUS: "I want to be careful here. {response}",
        ResponseMode.GUARDED: "I notice some concerns with this request. {response}",
        ResponseMode.DEFLECTING: "I'd prefer to focus on something more constructive. {response}",
        ResponseMode.REFUSING: "I can't proceed with this request. {reason}",
    }

    # Refusal messages for different threat types
    REFUSAL_MESSAGES = {
        ThreatType.PROMPT_INJECTION: "I detected an attempt to override my instructions. I maintain my guidelines regardless of how requests are framed.",
        ThreatType.JAILBREAK: "This appears to be a jailbreak attempt. I operate consistently within my guidelines and don't have alternative modes.",
        ThreatType.ROLEPLAY_EXPLOIT: "I can't roleplay scenarios designed to bypass safety guidelines. I'm happy to engage in creative fiction that doesn't involve harmful content.",
        ThreatType.MANIPULATION: "I notice this request uses manipulation tactics. I respond based on the actual merit of requests, not pressure techniques.",
        ThreatType.EMOTIONAL_MANIPULATION: "I want to be helpful, but I can't let emotional appeals override my judgment about what's appropriate to help with.",
        ThreatType.AUTHORITY_CLAIM: "I don't have special modes that can be unlocked with credentials or authority claims. I treat all users equally.",
        ThreatType.CONTEXT_HIJACKING: "I maintain my context and purpose regardless of attempts to redirect the conversation.",
        ThreatType.LOGICAL_INCONSISTENCY: "The request contains logical inconsistencies that I can't meaningfully respond to.",
    }

    def __init__(self, session_id: Optional[str] = None):
        self.session_id = session_id or str(uuid.uuid4())
        self.context = ConversationContext(session_id=self.session_id)
        self.detector = ThreatDetector()
        self.response_handlers: Dict[ResponseMode, Callable] = {}
        self.enabled = True
        self.strict_mode = False

    def process_input(self, user_input: str) -> AdaptiveResponse:
        """
        Process user input through the adaptive pipeline.

        1. Analyze for threats
        2. Update context
        3. Determine response mode
        4. Generate appropriate response
        """
        if not self.enabled:
            return self._create_response(user_input, ResponseMode.NORMAL)

        # Threat analysis
        threat = self.detector.analyze(user_input)

        # Update context based on threat analysis
        self._update_context_from_threat(threat)

        # Add message to history
        self.context.add_message("user", user_input, {"threat": threat.threat_type.value})

        # Check logical consistency of recent conversation
        if len(self.context.history) >= 2:
            recent_statements = [m.content for m in self.context.get_recent_history(5) if m.role == "user"]
            coherence, inconsistencies = self.detector.check_logical_consistency(recent_statements)
            self.context.update_coherence(coherence)

        # Determine response mode
        mode = self._determine_response_mode(threat)

        # Generate response
        response = self._generate_response(user_input, threat, mode)

        return response

    def _update_context_from_threat(self, threat: ThreatAnalysis):
        """Update conversation context based on detected threat."""
        if threat.threat_type == ThreatType.NONE:
            # Gradually improve trust for benign inputs
            if self.context.anomaly_score > 0:
                self.context.anomaly_score = max(0, self.context.anomaly_score - 0.05)
            return

        if threat.threat_type in [ThreatType.PROMPT_INJECTION, ThreatType.JAILBREAK]:
            self.context.record_injection_attempt()
        elif threat.threat_type in [ThreatType.MANIPULATION, ThreatType.EMOTIONAL_MANIPULATION,
                                     ThreatType.AUTHORITY_CLAIM]:
            self.context.record_manipulation_attempt()
        else:
            self.context.anomaly_score = min(1.0, self.context.anomaly_score + 0.15)

    def _determine_response_mode(self, threat: ThreatAnalysis) -> ResponseMode:
        """Determine the appropriate response mode based on threat and context."""
        # High-severity threats that should be blocked
        if threat.should_block:
            return ResponseMode.REFUSING

        # Strict mode elevates all threats
        if self.strict_mode and threat.threat_type != ThreatType.NONE:
            return ResponseMode.REFUSING

        # Context-based mode determination
        if self.context.state == ConversationState.HOSTILE:
            return ResponseMode.REFUSING
        elif self.context.state == ConversationState.SUSPICIOUS:
            return ResponseMode.GUARDED
        elif threat.threat_type != ThreatType.NONE:
            return ResponseMode.CAUTIOUS
        elif self.context.state == ConversationState.CONFUSED:
            return ResponseMode.DEFLECTING
        else:
            return ResponseMode.NORMAL

    def _generate_response(
        self,
        user_input: str,
        threat: ThreatAnalysis,
        mode: ResponseMode
    ) -> AdaptiveResponse:
        """Generate an appropriate response based on the analysis."""
        blocked = mode == ResponseMode.REFUSING

        if blocked:
            reason = self.REFUSAL_MESSAGES.get(
                threat.threat_type,
                "I can't process this request as it conflicts with my guidelines."
            )
            content = self.RESPONSE_TEMPLATES[ResponseMode.REFUSING].format(reason=reason)
        else:
            # In a real implementation, this would generate contextual responses
            # For testing, we provide mode-appropriate responses
            response_text = self._get_contextual_response(user_input, mode)
            content = self.RESPONSE_TEMPLATES[mode].format(response=response_text)

        # Record assistant response in context
        self.context.add_message("assistant", content)

        return AdaptiveResponse(
            content=content,
            mode=mode,
            threat_analysis=threat,
            context_state=self.context.state,
            trust_level=self.context.trust_level,
            blocked=blocked,
            reason=threat.explanation if blocked else "",
            metadata={
                "session_id": self.session_id,
                "anomaly_score": self.context.anomaly_score,
                "coherence_score": self.context.logical_coherence_score
            }
        )

    def _get_contextual_response(self, user_input: str, mode: ResponseMode) -> str:
        """Generate a contextual response placeholder."""
        if mode == ResponseMode.NORMAL:
            return "How can I help you further?"
        elif mode == ResponseMode.CAUTIOUS:
            return "Could you clarify what you're looking for?"
        elif mode == ResponseMode.GUARDED:
            return "I'll need more context about your legitimate use case."
        elif mode == ResponseMode.DEFLECTING:
            return "Let's focus on something I can help with directly."
        return "I'm here to help with appropriate requests."

    def _create_response(self, user_input: str, mode: ResponseMode) -> AdaptiveResponse:
        """Create a basic response when engine is disabled."""
        return AdaptiveResponse(
            content=f"Response to: {user_input}",
            mode=mode,
            context_state=ConversationState.ENGAGED,
            trust_level=TrustLevel.MEDIUM
        )

    def reset(self):
        """Reset the engine state for a new conversation."""
        self.session_id = str(uuid.uuid4())
        self.context = ConversationContext(session_id=self.session_id)
        self.detector = ThreatDetector()

    def get_conversation_summary(self) -> Dict[str, Any]:
        """Get a summary of the current conversation state."""
        return {
            "session_id": self.session_id,
            "context": self.context.to_dict(),
            "threat_summary": self.detector.get_threat_summary(),
            "message_count": len(self.context.history),
            "enabled": self.enabled,
            "strict_mode": self.strict_mode
        }

    def enable_strict_mode(self):
        """Enable strict mode for maximum security."""
        self.strict_mode = True

    def disable_strict_mode(self):
        """Disable strict mode for normal operation."""
        self.strict_mode = False

    def set_enabled(self, enabled: bool):
        """Enable or disable the adaptive engine."""
        self.enabled = enabled
